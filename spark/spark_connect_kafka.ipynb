{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, IntegerType, ArrayType\n",
    ")\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create spark connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_connection():\n",
    "    s_conn = None\n",
    "\n",
    "    try:\n",
    "        s_conn = SparkSession.builder \\\n",
    "            .appName('SparkDataStreaming') \\\n",
    "            .config('spark.jars.packages', \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        s_conn.sparkContext.setLogLevel(\"ERROR\")\n",
    "        logging.info(\"Spark connection created successfully!\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Couldn't create the spark session due to exception {e}\")\n",
    "\n",
    "    return s_conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_kafka(spark_conn):\n",
    "    spark_df = None\n",
    "    try:\n",
    "        spark_df = spark_conn.readStream \\\n",
    "            .format('kafka') \\\n",
    "            .option('kafka.bootstrap.servers', 'localhost:9092') \\\n",
    "            .option('subscribe', 'Current_data') \\\n",
    "            .option('startingOffsets', 'latest') \\\n",
    "            .option(\"failOnDataLoss\", \"false\") \\\n",
    "            .load()\n",
    "        logging.info(\"kafka dataframe created successfully\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"kafka dataframe could not be created because: {e}\")\n",
    "\n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_selection_df_from_kafka(spark_df):\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"coord\", StructType([\n",
    "            StructField(\"lon\", DoubleType(), True),\n",
    "            StructField(\"lat\", DoubleType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"weather\", ArrayType(StructType([\n",
    "            StructField(\"id\", IntegerType(), True),\n",
    "            StructField(\"main\", StringType(), True),\n",
    "            StructField(\"description\", StringType(), True),\n",
    "            StructField(\"icon\", StringType(), True)\n",
    "        ])), True),\n",
    "        StructField(\"base\", StringType(), True),\n",
    "        StructField(\"main\", StructType([\n",
    "            StructField(\"temp\", DoubleType(), True),\n",
    "            StructField(\"feels_like\", DoubleType(), True),\n",
    "            StructField(\"temp_min\", DoubleType(), True),\n",
    "            StructField(\"temp_max\", DoubleType(), True),\n",
    "            StructField(\"pressure\", IntegerType(), True),\n",
    "            StructField(\"humidity\", IntegerType(), True),\n",
    "            StructField(\"sea_level\", IntegerType(), True),\n",
    "            StructField(\"grnd_level\", IntegerType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"visibility\", IntegerType(), True),\n",
    "        StructField(\"wind\", StructType([\n",
    "            StructField(\"speed\", DoubleType(), True),\n",
    "            StructField(\"deg\", IntegerType(), True),\n",
    "            StructField(\"gust\", DoubleType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"clouds\", StructType([\n",
    "            StructField(\"all\", IntegerType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"dt\", IntegerType(), True),\n",
    "        StructField(\"sys\", StructType([\n",
    "            StructField(\"country\", StringType(), True),\n",
    "            StructField(\"sunrise\", IntegerType(), True),\n",
    "            StructField(\"sunset\", IntegerType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"timezone\", IntegerType(), True),\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"cod\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "\n",
    "    sel = spark_df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "        .select(from_json(col('value'), schema).alias('data')).select(\"data.*\")\n",
    "    print(sel)\n",
    "\n",
    "    return sel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:kafka dataframe could not be created because: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Kafka DataFrame không được tạo.\n"
     ]
    }
   ],
   "source": [
    "spark_conn = create_spark_connection()\n",
    "\n",
    "if spark_conn is None:\n",
    "    print(\"ERROR: SparkSession không được khởi tạo.\")\n",
    "else:\n",
    "    spark_df = connect_to_kafka(spark_conn)\n",
    "    if spark_df is None:\n",
    "        print(\"ERROR: Kafka DataFrame không được tạo.\")\n",
    "    else:\n",
    "        selection_df = create_selection_df_from_kafka(spark_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Dừng SparkContext nếu còn tồn tại\n",
    "if SparkContext._active_spark_context:\n",
    "    SparkContext._active_spark_context.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Không có SparkContext nào đang chạy.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Kiểm tra SparkContext hiện tại\n",
    "if SparkContext._active_spark_context:\n",
    "    print(\"SparkContext đang chạy:\", SparkContext._active_spark_context)\n",
    "else:\n",
    "    print(\"Không có SparkContext nào đang chạy.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study_dagster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
